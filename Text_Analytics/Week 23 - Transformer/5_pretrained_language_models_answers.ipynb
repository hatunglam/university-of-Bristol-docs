{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba36903-ee71-4a62-9bb8-8e0270fdae64",
   "metadata": {},
   "source": [
    "# Text Analytics Lab 5: Pretrained Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244c460-fa21-482a-a6e6-4216f8f74c24",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook introduces the Transformers library from HuggingFace, which we can use to access a wide range of pretrained language models. The sections are:\n",
    "\n",
    "   1. **Introducing Transformers:** This section introduces the Transformers library from HuggingFace, showing you how to use it to obtain contextualised embeddings from pretrained transformer models.\n",
    "   1. **Transformers for Text Classification:** Here we show you how to construct a classifier using Transformers.\n",
    "   1. **OPTIONAL: More on Transformers:** Some pointers to other materials if you want to learn more about transformers, e.g., if using them in your summer project. \n",
    "\n",
    "Example code for all the tasks has been tested on a four-year old MacBook Pro, and the longest training process took under 10 minutes. If you find that the code takes too long to run on your own machine, a good alternative is to use [Google Colab](https://colab.research.google.com/), Amazon Sagemaker Studio, or hte lab machines on campus. \n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "These sections will contain tutorial-like instructions, as you have seen in previous text analytics labs. On completing these sections, the intended learning outcomes are that you will be able to...\n",
    "1. Use pretrained transformers to obtain contextualised word and sentence embeddings.\n",
    "1. Apply a pretrained QA model to a new dataset. \n",
    "1. Construct classifiers with pretrained transformers. \n",
    "1. Find documentation on pretrained models in the Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3b707e-0ce9-49cb-b226-6ad387fddebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a9045-9d0b-4427-b91c-a085496de2a5",
   "metadata": {},
   "source": [
    "# 1. Introducing Transformers \n",
    "\n",
    "HuggingFace is a company that has developed an open source library for loading pretrained transformer models. They also distribute many models that have been pretrained using language modelling tasks, or fine-tuned to specific downstream NLP tasks.  It is currently the best library to use to create NLP models on top of large, deep neural networks. This is especially useful for tasks where simpler, feature-based methods or smaller LSTM models do not perform well enough, for example, when complex processing of syntax and semantics is required (natural language 'understanding'). \n",
    "\n",
    "The larger models often give great performance, but the trade-off is that they require a lot of memory and compute. When building a model for a new dataset, it is a good idea to compare faster models with transformers to determine whether the performance/cost trade-off is worth it on that particular dataset. \n",
    "\n",
    "Let's start by looking at two key types of object in the transformers library: models and tokenizers.\n",
    "\n",
    "## 1.1. Models\n",
    "\n",
    "The neural network models available in the Transformers library are accessed through wrapper classes such as `AutoModel`. If we want to load a pretrained model, we can simply pass its name to the `from_pretrained` function, and the pretrained model weights will be downloaded from HuggingFace and a neural network model will be created with those weights. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122d993d-e48c-4dc2-bc14-2949b3d67e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe226ec-580f-4022-97b0-e5010ddfb55e",
   "metadata": {},
   "source": [
    "This code loads the TinyBERT model, which is a compressed version of BERT. It has 4.4 million parameters, compared to the standard version of BERT, 'BERT-base', which has 110 million parameters. While TinyBERT will not perform as well as larger models, we will use it for this notebook to save memory and computation costs. See [documentation here](https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D).\n",
    "\n",
    "<!--the RoBERTa variant of BERT. It has 4.4 million parameters, compared to the standard version of BERT, 'BERT-base', which has 110 million parameters. While RoBERTa-tiny will not perform as well as larger models, we will use it for this notebook to save memory and computation costs. See [documentation here](https://huggingface.co/arampacha/roberta-tiny).  -->\n",
    "\n",
    "The same functions can be used to load other models from HuggingFace's repository simply by changing the model's name. Take a look at [the Models page](https://huggingface.co/models) so see what there is on offer. Do you recognise any of the models' names?\n",
    "\n",
    "# 1.2. Tokenizers\n",
    "\n",
    "Before we can apply a model to some text, we need to a create Tokenizer object. In Transfomers, Tokenizer objects convert raw text to a sequence of numbers. First, the tokenizer actually performs tokenization, then it maps each token to its numerical ID. There are lots of different tokenizers that we can use to preprocess text. If we are loading a pretrained model, we will need to choose the tokenizer that corresponds to that model. \n",
    "\n",
    "**TO-DO 1:** Why is it necessary to choose a matching tokenizer for a pretrained model?\n",
    "\n",
    "We can load the right tokenizer as follows, in the same way we loaded the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24395ef-aa4a-4d62-a6ea-aa97d86f42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ed12c-cb5a-48c5-b67f-7d17b0a79514",
   "metadata": {},
   "source": [
    "Let's see what the TinyBERT tokenizer does to an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e5852f-46b3-4e28-b952-780f472b6a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'transform', '##er', 'architecture', 'has', 'transformed', 'the', 'field', 'of', 'nl', '##p', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The transformer architecture has transformed the field of NLP.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807c966-188f-4a0b-b435-4282d5aa0201",
   "metadata": {},
   "source": [
    "Let's compare with the NLTK tokenizer we have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee63d96e-2441-4591-bff9-3a1bf6289828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'transformer', 'architecture', 'has', 'transformed', 'the', 'field', 'of', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(sentence)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f2123-34a6-4095-9981-2966fdadd7d8",
   "metadata": {},
   "source": [
    "While NLTK keeps whole words as tokens, the BERT tokenizer splits some words into sub-words and inserts some special characters into the tokens. Splitting is applied to words with low frequency in the training set, such as 'transformer'. \n",
    "\n",
    "Rather than following a set of hand-crafted rules, the BERT tokenizer is learned from a large dataset. It starts by adding individual characters to its vocabulary. Then, it adds the most frequently occurring pairs of characters as tokens in the vocabulary. This repeats by adding the most frequent pairs of tokens to the vocabulary until the desired size of dictionary is reached. When tokenizing a document, words that are not in the vocabulary are matched against the shorter sub-word tokens.\n",
    "\n",
    "**TO-DO 2:** What is the benefit of splitting some words into sub-word tokens? \n",
    "\n",
    "WRITE YOUR ANSWER HERE.\n",
    "\n",
    "Rare/out-of-vocabulary words can often be broken into constituent parts, like stems/root forms of a verb, suffixes, prefixes, and other parts of words. The meaning can be composed from these parts, and these parts may convey syntactic or semantic information themselves that is useful for processing the whole sentence. Using subwords to represent rare words also allows us to limit vocabulary size.\n",
    "\n",
    "---\n",
    "\n",
    "After tokenization, the Tokenizer object can also map the tokens to their IDs (indexes in the vocabulary), so that we can pass them as input to a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950440ae-d58f-4675-8fb4-6114462331fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1996, 10938, 2121, 4294, 2038, 8590, 1996, 2492, 1997, 17953, 2361, 1012]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab127d8-6e03-41bb-a646-91f8ef83ae20",
   "metadata": {},
   "source": [
    "Let's load up a dataset that we can use for our experiments later on. We will use the [TweetEval hate speech](https://huggingface.co/datasets/tweet_eval) dataset to train and test a classifier. The task is to classify tweets into one of  0: non-hate or 1: hate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcd5efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 9000 instances loaded\n",
      "Validation dataset with 1000 instances loaded\n",
      "Test dataset with 2970 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#cache_dir = './data_cache/'\n",
    "\n",
    "# Load up the emotion dataset...\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"hate\",\n",
    "    split=\"train\",\n",
    "    #cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"hate\",\n",
    "    split=\"validation\",\n",
    "    #cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Validation dataset with {len(val_dataset)} instances loaded\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"hate\",\n",
    "    split=\"test\",\n",
    "    #cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226758a-82fb-4b5a-bc27-83a71abbf1b9",
   "metadata": {},
   "source": [
    "Now, let's see apply our tokenizer to the dataset, using the map function to run it on all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b94acc-77ff-434e-99d0-ec49f00fa58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e680224aed01428cba00af60bfe2b63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize...\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    model_inputs = tokenizer(dataset['text'], padding=\"max_length\", max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5bc90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 9000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17f77c-46f4-4ab0-9eee-858fd0b237f2",
   "metadata": {},
   "source": [
    "## 1.3. Contextualised Embeddings\n",
    "\n",
    "Now that we have a sequence of tokens, we are almost ready to process the sequence using the pretrained model. \n",
    "\n",
    "Our model takes as input a PyTorch `tensor` (a muli-dimensional array). Here, we need a two-dimensional matrix, where each row is a sequence of input tokens corresponding to a single sentence or document. Let's convert our list of IDs to a 2-D tensor with a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c333937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07007371-2125-4fb0-8511-63ade9a2f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1996, 10938,  2121,  4294,  2038,  8590,  1996,  2492,  1997, 17953,\n",
      "          2361,  1012]])\n"
     ]
    }
   ],
   "source": [
    "ids_tensor = torch.tensor([ids])\n",
    "\n",
    "print(ids_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508047c1-96fa-4839-9eb3-8e64e91c5919",
   "metadata": {},
   "source": [
    "Now we can process the sequence using our model. The pretrained transformer model maps the sequence of input IDs to a sequence of output vectors, which are contextualised word embeddings. The hidden state values produced in the last hidden layer of the model are used as the contextualised embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "494c8335-7c8a-47c2-b450-a3b775f2262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complete model outputs: \n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3608,  0.2862, -0.1549,  ..., -0.2064,  0.2663, -0.0109],\n",
      "         [ 0.0149,  0.7223, -0.0508,  ..., -0.5505,  0.2355, -0.2962],\n",
      "         [ 0.1531,  0.5903, -0.1244,  ..., -0.4263,  0.0417, -0.1839],\n",
      "         ...,\n",
      "         [ 0.1742, -0.1091, -0.1963,  ..., -0.6736,  0.0472, -0.1840],\n",
      "         [ 0.2434,  0.1021, -0.2241,  ..., -0.5400, -0.1691, -0.1314],\n",
      "         [ 0.0854,  0.3272, -0.3016,  ..., -0.2154, -0.5632, -0.1921]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.1380e-02, -6.3005e-03,  1.8521e-02,  7.1139e-03, -3.1795e-02,\n",
      "          1.3882e-02, -1.5459e-02, -1.0610e-03, -1.8263e-02, -3.6515e-02,\n",
      "         -2.1257e-02, -1.5479e-02, -2.8094e-04, -4.1092e-02, -2.5315e-02,\n",
      "         -4.3338e-02, -1.1617e-03, -1.3931e-02,  6.0733e-03,  4.3790e-03,\n",
      "          2.7094e-04, -2.1810e-02, -4.8026e-02,  2.5493e-02, -1.6502e-02,\n",
      "         -1.2034e-03,  4.2757e-02,  3.0715e-03, -2.3439e-02, -2.5849e-02,\n",
      "         -1.1970e-02,  3.4340e-02, -1.9025e-04, -2.9000e-03, -4.1984e-02,\n",
      "          2.2932e-02,  4.0502e-02,  2.0257e-02, -3.0206e-03, -1.5759e-02,\n",
      "         -4.6124e-02,  6.4128e-03, -1.3939e-02,  4.7155e-02, -2.6471e-03,\n",
      "          3.4992e-03, -4.6632e-03,  8.5902e-03, -5.5664e-02,  9.6635e-03,\n",
      "         -1.9049e-02, -2.3983e-02,  6.6501e-03, -8.3702e-03, -1.4176e-02,\n",
      "          5.6312e-03,  4.2979e-03,  2.4337e-02, -5.4482e-04,  1.1454e-02,\n",
      "         -1.0526e-02, -2.5978e-02, -2.8789e-03, -1.9640e-02, -7.0220e-03,\n",
      "          1.9534e-02,  1.2682e-02, -5.3417e-02, -2.8265e-03,  7.8937e-03,\n",
      "         -4.0802e-02,  3.5494e-02, -8.6865e-03, -1.3383e-02,  1.6433e-03,\n",
      "          5.4637e-03,  1.8293e-02, -1.7385e-02,  3.4079e-02, -3.3782e-02,\n",
      "         -1.4324e-02,  3.0941e-02, -2.2349e-02, -1.4385e-02,  1.6196e-03,\n",
      "          2.3094e-02, -2.1322e-02,  2.5966e-02, -1.5705e-02, -2.1027e-03,\n",
      "         -1.5518e-02,  2.8620e-02,  1.5401e-02, -3.7512e-02, -6.7670e-03,\n",
      "         -8.8314e-03, -1.0061e-02, -1.0518e-02, -1.4193e-02,  4.8184e-02,\n",
      "         -4.2733e-02, -6.8400e-04,  8.1719e-03,  6.9233e-03, -2.9019e-04,\n",
      "         -1.6003e-02, -5.5384e-03,  8.2021e-03,  1.7942e-02, -2.9106e-02,\n",
      "          3.7280e-02,  2.1304e-04,  4.1467e-02, -3.1194e-02, -2.8117e-02,\n",
      "         -1.1518e-02,  4.7534e-02, -2.4884e-02, -2.2532e-02,  1.1021e-02,\n",
      "          4.2616e-02, -2.3387e-02,  3.1372e-02,  2.8327e-03,  9.1082e-03,\n",
      "          2.9967e-02,  1.8474e-02, -6.1313e-02,  1.1888e-02, -1.0234e-02,\n",
      "         -1.7436e-02, -2.5381e-03, -2.0706e-02,  6.2739e-03,  3.5936e-02,\n",
      "         -2.7933e-02,  2.7344e-02,  5.7079e-02, -1.6678e-02,  2.4630e-02,\n",
      "          2.6223e-02,  6.1135e-03,  1.2738e-02, -3.5122e-02, -6.0352e-02,\n",
      "          3.7475e-02, -2.0558e-02, -1.1147e-02, -2.5519e-02, -1.6405e-02,\n",
      "         -5.6022e-03,  3.6427e-02, -3.9645e-03,  6.1233e-03,  1.0637e-02,\n",
      "          2.0472e-02,  3.1075e-03,  1.8298e-02, -7.4328e-03,  1.4522e-02,\n",
      "         -3.2960e-02,  4.0197e-02, -4.9703e-02,  1.5634e-02,  3.3699e-02,\n",
      "          4.1567e-02,  3.4694e-02, -3.8650e-02,  4.7024e-02,  4.5113e-03,\n",
      "         -3.7264e-02, -3.8624e-02, -1.5380e-02,  9.4787e-03, -5.9185e-04,\n",
      "          2.2612e-02,  1.6623e-02,  3.8173e-02,  1.8445e-02, -2.8130e-02,\n",
      "         -8.9479e-04,  9.6279e-03,  1.0077e-02, -3.0724e-02,  2.3116e-02,\n",
      "          2.2716e-02, -2.7906e-02, -2.4386e-02,  6.4444e-03,  4.0045e-02,\n",
      "         -3.8941e-02, -1.3221e-03,  3.2294e-02,  1.9687e-02,  1.5109e-03,\n",
      "          2.2807e-03,  1.1437e-02,  2.1764e-03,  2.4419e-02,  2.5356e-02,\n",
      "         -3.9937e-02,  3.6392e-02, -1.4530e-03,  4.5135e-03, -3.1404e-02,\n",
      "         -2.7645e-02,  3.7107e-02, -2.8359e-02,  2.1804e-02, -2.8085e-02,\n",
      "          2.3142e-02, -3.7059e-02, -3.2632e-02, -3.4501e-02, -5.5987e-03,\n",
      "          4.1899e-03,  6.4632e-04, -4.4314e-02, -4.9819e-02,  2.1470e-02,\n",
      "          1.3016e-02,  1.4301e-02,  1.8736e-02, -5.1866e-03,  1.5026e-02,\n",
      "         -2.1911e-05,  3.7323e-02, -4.4686e-02,  2.5535e-02, -1.6726e-02,\n",
      "          7.3110e-03,  4.8137e-05, -4.0820e-03, -1.7965e-03,  2.8757e-02,\n",
      "          5.1982e-02, -3.4868e-02,  2.4777e-02, -6.2330e-03, -1.9709e-02,\n",
      "         -2.1922e-02,  3.1252e-02,  1.3890e-02, -1.7581e-02,  3.5594e-02,\n",
      "          4.8341e-03,  2.7993e-03, -2.6515e-02, -1.4557e-02, -3.1771e-02,\n",
      "          4.0668e-02,  3.3960e-02,  1.6408e-02,  3.1032e-03, -1.5013e-02,\n",
      "         -9.6832e-03, -1.3780e-02, -3.2977e-02,  1.1426e-02, -4.0550e-03,\n",
      "         -2.4083e-03,  3.5824e-02, -4.9809e-02,  3.7798e-02,  1.7321e-02,\n",
      "          1.0082e-02,  5.8975e-02, -4.2596e-02,  3.6302e-02, -1.2863e-02,\n",
      "         -3.1070e-02,  1.3231e-03, -2.5990e-02, -1.7098e-02, -1.9114e-02,\n",
      "          6.7506e-03,  1.4090e-02,  2.7720e-02,  2.3714e-02, -1.8923e-02,\n",
      "         -2.0685e-02, -4.4664e-02, -1.6670e-02, -1.3620e-02,  1.5473e-02,\n",
      "         -4.4576e-03,  7.4258e-04, -3.9406e-03,  1.4169e-02, -2.6952e-03,\n",
      "         -2.6419e-02,  2.4067e-02,  4.7123e-02,  8.8112e-03,  1.7120e-02,\n",
      "          1.0671e-02, -2.4988e-03,  4.1719e-03, -3.7655e-02,  1.7643e-02,\n",
      "          1.0087e-02, -9.5763e-03, -2.1317e-02, -4.8866e-02, -1.9793e-02,\n",
      "          2.8578e-02,  1.2889e-02, -1.9376e-02,  4.6893e-02, -8.4308e-04,\n",
      "          1.6833e-03,  4.6231e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "The last hidden state sequence for the first sentence in our batch (we only have one sentence in the batch): \n",
      "tensor([[ 0.3608,  0.2862, -0.1549,  ..., -0.2064,  0.2663, -0.0109],\n",
      "        [ 0.0149,  0.7223, -0.0508,  ..., -0.5505,  0.2355, -0.2962],\n",
      "        [ 0.1531,  0.5903, -0.1244,  ..., -0.4263,  0.0417, -0.1839],\n",
      "        ...,\n",
      "        [ 0.1742, -0.1091, -0.1963,  ..., -0.6736,  0.0472, -0.1840],\n",
      "        [ 0.2434,  0.1021, -0.2241,  ..., -0.5400, -0.1691, -0.1314],\n",
      "        [ 0.0854,  0.3272, -0.3016,  ..., -0.2154, -0.5632, -0.1921]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_outputs = model(ids_tensor)\n",
    "print('The complete model outputs: ')\n",
    "print(model_outputs)\n",
    "\n",
    "print()\n",
    "print('The last hidden state sequence for the first sentence in our batch (we only have one sentence in the batch): ')\n",
    "embeddings = model_outputs['last_hidden_state'][0]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee4181-7518-484f-993d-f7abffcae010",
   "metadata": {},
   "source": [
    "We can retrieve the embedding vector for \"transform\" like this (\"transform\" is the second token in the sequence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3df76db-f81e-4916-b31d-34a8341a20db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.49152540e-02  7.22318172e-01 -5.07856756e-02 -2.74205208e-01\n",
      " -1.38931945e-01  1.00099719e+00  7.11460225e-03  2.71391630e-01\n",
      " -3.92814502e-02  6.04100786e-02  1.25740275e-01  4.60631132e-01\n",
      "  6.25288114e-03  1.61929965e-01  1.23913512e-01 -4.08096790e-01\n",
      "  1.24868281e-01 -4.71536934e-01  2.24768654e-01  6.35188073e-02\n",
      "  8.56176019e-02 -1.88044831e-01  1.77257672e-01  3.40048403e-01\n",
      " -1.95545748e-01  1.58553362e-01  9.62866545e-02  1.12649694e-01\n",
      "  2.21045166e-01 -9.56114054e-01 -3.85948956e-01  1.39220521e-01\n",
      "  5.90011775e-01 -8.06727529e-01 -1.34288132e-01  2.35691771e-01\n",
      " -1.02274150e-01  2.78303713e-01  7.94321418e-01 -2.49362856e-01\n",
      "  1.72771528e-01 -2.07582936e-01  3.00157130e-01 -8.59332681e-02\n",
      " -2.25284532e-01 -9.75404009e-02 -3.52349520e-01  3.81161213e-01\n",
      " -3.87681633e-01 -1.77613512e-01 -4.13684934e-01  1.38047546e-01\n",
      "  1.29874498e-02  6.52685225e-01  1.16502643e-01 -5.10779560e-01\n",
      " -8.30415636e-02 -2.67047882e-02  3.12862933e-01 -2.62848467e-01\n",
      " -1.43285245e-01  1.10270180e-01 -1.46888584e-01  8.40608925e-02\n",
      " -5.41600466e-01 -2.74967905e-02 -2.71179706e-01  3.61281395e-01\n",
      "  1.86117202e-01 -2.27869272e-01 -8.60140473e-02  4.15634781e-01\n",
      " -2.35568900e-02  2.90852666e-01 -4.29376662e-01  5.02850235e-01\n",
      "  4.46188599e-01 -3.45921427e-01 -1.07707895e-01  9.98826474e-02\n",
      "  4.75118637e-01  1.42710701e-01  6.47673383e-03  1.91742964e-02\n",
      "  4.32648361e-01 -7.28579164e-02  1.23887107e-01  4.31221247e-01\n",
      " -2.87044764e-01  8.61874580e-01 -3.77015501e-01 -2.96231538e-01\n",
      " -6.53625652e-02 -1.08727545e-01 -5.34366667e-01  2.30699763e-01\n",
      " -4.43281680e-02  8.28478485e-03 -4.80108917e-01  6.32635728e-02\n",
      "  3.46862406e-01 -1.00396685e-02  2.46484190e-01  1.04728386e-01\n",
      "  5.85354343e-02 -1.73107013e-01 -1.63396329e-01 -1.42520756e-01\n",
      " -1.41768932e-01  5.59849404e-02  2.56438196e-01 -3.92890930e-01\n",
      " -3.47414166e-01 -5.97775877e-01 -1.29222408e-01 -3.44478935e-01\n",
      "  7.78063089e-02  2.47690827e-01  3.36303353e-01 -1.69770271e-01\n",
      "  5.26935309e-02 -4.30812597e-01 -7.06477165e-01  2.43624132e-02\n",
      " -2.37541467e-01 -3.28975797e-01  1.46830320e-01  4.63139743e-01\n",
      " -7.27381706e-02  4.38396595e-02  4.04201634e-02 -3.49174552e-02\n",
      " -5.20073473e-02 -1.70057163e-01  3.05908006e-02  9.56189185e-02\n",
      "  1.73355949e+00  2.14067847e-01  6.37322888e-02  5.80558717e-01\n",
      " -9.82592702e-02 -4.88570869e-01  2.82071501e-01  2.64478922e-01\n",
      "  3.23805809e-01 -2.94938385e-01  1.32838950e-01 -3.09572425e-02\n",
      "  1.98950484e-01  1.01505816e-01  3.46358269e-01  4.12193000e-01\n",
      " -2.06010461e-01  1.38656273e-01  8.95758718e-02 -9.03494060e-02\n",
      "  1.60656124e-02  1.04299173e-01  5.76754272e-01  4.83328164e-01\n",
      "  1.27745941e-01 -4.36361954e-02  2.58013904e-01 -7.33279884e-02\n",
      " -3.81113410e-01 -4.86678854e-02  5.54443538e-01 -2.71808058e-02\n",
      " -3.28379929e-01  1.03766024e-01 -7.67528564e-02 -2.55299330e-01\n",
      " -4.58066583e-01 -6.74318820e-02 -5.92072755e-02 -5.33048473e-02\n",
      "  7.31045008e-01 -4.37691174e-02  1.48427173e-01 -6.68218136e-01\n",
      "  5.39264143e-01 -5.51167727e-01  3.07089090e-01 -2.95280933e-01\n",
      "  2.30028540e-01  4.45130229e-01  5.98995537e-02  3.32110345e-01\n",
      "  3.09372783e-01  7.49412894e-01  6.80033863e-01  2.81823754e-01\n",
      " -1.52892828e-01 -2.78219551e-01  1.57606781e-01 -3.44310068e-02\n",
      " -4.25324887e-01  3.65775973e-01 -6.05321042e-02 -2.44895399e-01\n",
      " -9.54142213e-02  2.14866459e-01  3.48824710e-02  4.22627270e-01\n",
      "  1.17061734e-01  1.53852731e-01  4.89413172e-01 -4.79473770e-02\n",
      " -4.35449719e-01  1.52307749e-01 -3.75602186e-01 -2.00070232e-01\n",
      " -4.52389896e-01 -9.23973918e-02  7.94968382e-02 -3.91880453e-01\n",
      "  2.03301743e-01  5.86735010e-01  1.18395425e-02 -3.49965602e-01\n",
      " -1.03540495e-02  1.67384431e-01  1.03552327e-01  1.20476186e-02\n",
      "  2.16728508e-01 -6.50186241e-02 -3.98292065e-01 -2.12185144e-01\n",
      "  1.36510730e-02 -5.55399954e-01 -2.92275518e-01  1.41718343e-01\n",
      "  5.35454273e-01  2.17892140e-01 -4.19899285e-01 -3.32854986e-01\n",
      "  3.76725972e-01 -3.19370836e-01 -3.27616692e-01  4.93491411e-01\n",
      "  1.59246370e-01  7.08294511e-01 -1.90098062e-01  5.06710470e-01\n",
      " -6.10249460e-01 -6.68388456e-02  1.14125800e+00  8.79680887e-02\n",
      " -1.85307801e-01 -1.45168751e-01  2.87572384e-01 -4.92560387e-01\n",
      "  3.61905135e-02  2.62836993e-01 -2.52178282e-01  6.15251660e-02\n",
      " -3.54110003e-01 -3.53945613e-01  1.98717892e-01  1.07959211e+00\n",
      "  1.29683340e+00  3.58777076e-01  6.75654948e-01  2.51169294e-01\n",
      " -5.65680087e-01 -4.81561691e-01  3.10951531e-01 -1.07663020e-01\n",
      " -3.67722720e-01  8.12462866e-02 -4.20884520e-01  5.78672588e-01\n",
      "  9.49736536e-02  1.65470511e-01  3.70412439e-01 -7.44807394e-03\n",
      "  4.63329315e-01 -2.38428652e-01  4.81723957e-02 -1.74412131e-01\n",
      "  8.61781612e-02 -1.35658771e-01  2.68295497e-01  2.98455328e-01\n",
      "  6.36236131e-01  2.79196173e-01 -1.57060608e-01 -2.91060358e-01\n",
      " -1.38483077e-01 -9.19544548e-02 -3.60318422e-01 -3.03814590e-01\n",
      "  2.38155782e-01  3.61209244e-01  1.53050408e-01  5.15775792e-02\n",
      " -1.36736780e-03  9.57003906e-02 -3.30190539e-01  3.86730522e-01\n",
      " -2.75556207e-01  3.30926389e-01 -2.23883897e-01 -4.71362799e-01\n",
      "  8.31371620e-02 -2.26590872e-01  2.15096548e-01  2.63797909e-01\n",
      "  3.72822315e-01 -5.50537348e-01  2.35453576e-01 -2.96163768e-01]\n",
      "The TinyBERT embeddings have 312 dimensions.\n"
     ]
    }
   ],
   "source": [
    "emb = embeddings[1]  # get second embedding in the sequence\n",
    "\n",
    "# convert it to a numpy array so we can perform various operations on it later on\n",
    "emb = emb.detach().numpy()\n",
    "\n",
    "print(emb)\n",
    "print(f'The TinyBERT embeddings have {emb.shape[0]} dimensions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1d779-8ec7-462a-895b-e58a88d9bb63",
   "metadata": {},
   "source": [
    "**TO-DO 3:** Retrieve the embedding for \"architecture\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827ca93a-d80d-4655-a417-6c6c6d1e07f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.7139e-01,  7.7458e-01, -3.2426e-01, -7.1433e-02, -4.9510e-04,\n",
      "         9.3731e-01, -4.4026e-03, -4.2692e-02,  1.2740e-02,  1.8927e-02,\n",
      "         1.0253e-01,  4.5466e-01,  2.7044e-01,  2.3099e-01,  4.0370e-03,\n",
      "        -1.0899e-01, -4.5991e-02, -3.5115e-01, -1.3471e-01,  8.2940e-02,\n",
      "         1.8650e-01,  5.0027e-02,  7.2166e-02,  2.2866e-01, -2.1970e-01,\n",
      "         9.4020e-02,  1.6554e-01,  1.8579e-01,  3.1778e-01, -5.0937e-01,\n",
      "        -5.0095e-01,  1.5249e-01,  4.5800e-01, -8.5188e-01, -1.5863e-01,\n",
      "         1.5896e-01,  4.1620e-02,  2.3100e-01,  8.7850e-01, -6.2316e-02,\n",
      "         1.8722e-01, -1.2338e-02,  2.1008e-01,  3.4806e-02, -2.5124e-01,\n",
      "        -1.3791e-01, -3.8870e-01,  2.9819e-01, -2.9203e-01, -3.1950e-01,\n",
      "        -1.9843e-01,  1.3203e-01, -6.4638e-02,  7.4318e-01,  7.1424e-02,\n",
      "        -3.0212e-01,  3.4978e-01, -5.8179e-02,  2.8507e-01, -4.0958e-01,\n",
      "        -1.0330e-01,  1.0377e-01, -2.2290e-01,  8.8632e-02, -4.3360e-01,\n",
      "         2.1787e-01, -2.7699e-01,  3.9959e-01,  1.6920e-01, -3.0519e-01,\n",
      "        -2.6639e-01,  7.2838e-01, -2.1169e-01,  3.9785e-01, -5.8909e-01,\n",
      "         4.2977e-01,  4.2544e-01, -2.1426e-01, -9.1709e-02,  2.1237e-01,\n",
      "         4.3058e-01, -1.4151e-01,  2.1101e-02,  1.9252e-01,  3.8790e-01,\n",
      "        -2.2849e-01,  2.3207e-01,  4.9791e-01, -1.2262e-01,  5.9461e-01,\n",
      "        -3.7332e-01, -3.3465e-01, -1.7153e-01, -1.5562e-01, -6.5699e-01,\n",
      "         1.6303e-01, -1.9850e-02, -1.4620e-01, -4.4782e-01,  1.8682e-01,\n",
      "         2.5921e-01,  8.5929e-02,  3.3952e-01, -1.2985e-01, -1.1409e-01,\n",
      "        -2.0028e-01, -2.1566e-01, -5.6376e-02, -1.0793e-01, -4.6821e-02,\n",
      "         2.5686e-01, -2.7753e-01, -2.1115e-02, -5.3542e-01, -7.2415e-02,\n",
      "        -4.1341e-01,  1.7352e-01,  1.1312e-01,  2.3880e-01, -2.2446e-01,\n",
      "         8.6292e-02,  1.1534e-02, -3.6239e-01,  2.2901e-01, -3.5063e-01,\n",
      "        -3.9716e-01, -5.6411e-03,  5.4328e-01,  4.4247e-02,  2.7485e-01,\n",
      "        -4.3290e-02, -8.1752e-02,  2.2652e-01, -2.1781e-01, -2.0980e-01,\n",
      "         3.6805e-01,  1.7491e+00,  2.3244e-01,  4.1200e-02,  4.2876e-01,\n",
      "        -7.7350e-02, -3.9446e-01,  4.5622e-01,  3.9475e-01,  3.4798e-01,\n",
      "        -1.9866e-01,  1.6629e-02,  4.1721e-02,  2.8072e-01,  1.7268e-01,\n",
      "         3.7179e-01,  4.5893e-01, -2.0369e-01,  2.0881e-01,  1.3248e-01,\n",
      "         4.3476e-03, -1.4810e-01,  1.1155e-01,  5.0559e-01,  6.2457e-01,\n",
      "         5.5675e-02, -1.9302e-01,  1.4475e-01, -2.4213e-01, -3.7723e-01,\n",
      "        -8.8337e-02,  3.1863e-01,  1.2931e-03, -4.6815e-01,  1.0901e-01,\n",
      "        -9.8819e-02,  4.3396e-02, -5.9569e-01, -6.2998e-02,  1.9007e-01,\n",
      "        -6.2226e-02,  4.4438e-01, -4.4506e-02,  3.6364e-02, -7.4973e-01,\n",
      "         6.0642e-01, -5.9444e-01,  2.2702e-01, -3.2453e-01,  2.2776e-01,\n",
      "         2.5375e-01,  3.3793e-02,  3.5470e-01,  1.4837e-01,  8.0149e-01,\n",
      "         7.3767e-01,  3.1693e-01, -4.6550e-01, -2.2000e-01,  8.5513e-02,\n",
      "         7.9039e-02, -4.2197e-01,  3.0032e-01, -1.6429e-02, -3.0500e-01,\n",
      "        -1.2967e-01,  5.9773e-02,  2.2780e-01,  2.5144e-01,  1.3294e-01,\n",
      "         1.7734e-01,  5.3502e-01, -1.2685e-01, -3.7097e-01,  1.9789e-01,\n",
      "        -5.8195e-01, -8.7312e-02, -4.3854e-01,  4.8150e-02,  4.2731e-02,\n",
      "        -3.5227e-01,  9.0655e-02,  8.3854e-01,  1.9043e-01, -3.2788e-01,\n",
      "         5.3903e-02,  2.0942e-01,  2.6107e-01,  1.4168e-01,  2.5795e-01,\n",
      "         1.1505e-01, -2.2946e-01, -3.1716e-01,  2.0359e-01, -4.9422e-01,\n",
      "        -1.8840e-01,  3.9925e-01,  3.6140e-01,  1.0215e-01, -6.3954e-01,\n",
      "        -4.5504e-01,  3.4861e-01, -3.0188e-01, -3.1184e-01,  5.3751e-01,\n",
      "         1.7970e-01,  9.5390e-01, -9.2565e-02,  2.9718e-01, -4.6427e-01,\n",
      "        -1.5498e-01,  1.0402e+00, -5.7101e-02, -1.2549e-01, -1.5998e-01,\n",
      "         3.0259e-01, -6.5434e-01,  8.3300e-02,  2.9659e-01, -2.8819e-01,\n",
      "        -5.1962e-02, -1.1606e-01, -4.0287e-01, -1.8567e-02,  8.5405e-01,\n",
      "         1.0442e+00,  2.0221e-01,  3.8742e-01,  2.7672e-01, -6.0419e-01,\n",
      "        -4.5675e-01,  1.9838e-01, -1.9707e-01, -2.9501e-01,  2.7894e-01,\n",
      "        -6.3120e-01,  5.5693e-01,  2.0348e-01,  9.1781e-02,  2.5457e-01,\n",
      "        -3.9542e-02,  2.6213e-01, -2.7572e-01,  3.5246e-01, -3.8520e-01,\n",
      "         2.1084e-01, -8.3348e-03,  2.2054e-01, -2.2407e-02,  5.5667e-01,\n",
      "         1.0101e-01, -7.7358e-02, -4.6361e-01, -3.2062e-01, -5.7820e-02,\n",
      "        -2.2930e-01, -2.7726e-01,  1.5233e-01,  4.0260e-01,  1.1090e-01,\n",
      "         1.8383e-01, -3.9561e-02,  1.6976e-02, -5.0557e-01,  6.3555e-02,\n",
      "         6.9837e-02,  5.7396e-01, -1.9718e-01, -3.7291e-01,  1.0182e-01,\n",
      "        -1.7266e-01,  2.1130e-01,  1.7053e-01,  2.3580e-01, -4.7386e-01,\n",
      "        -1.3976e-01, -2.5248e-01], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR ANSWER HERE\n",
    "\n",
    "print(embeddings[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c79402-f6b7-4fc3-9f47-50fcffccbe68",
   "metadata": {},
   "source": [
    "Sentences and documents usually have varying lengths. So, to put multiple sentences into a single tensor, we need to pad the sequences up to a maximum length. Luckily, the tokenizer class takes care of this for us. When we pass in a list of sentences, the tokenizer creates a matrix, where each row is a sequence of the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03abaed-6738-44ab-a7dd-7775743f99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2064,  2338,  9735,  2005,  1996,  4164,  2279,  2733,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2116,  8141,  2424,  1996,  2034,  2338,  1997,  1037,  6925,\n",
      "          1997,  2048,  3655,  2000,  2022, 16801,  1012,   102],\n",
      "        [  101,  2016,  2441,  1996,  2338,  2000,  3931,  4261,  1998,  2211,\n",
      "          2000,  3191, 12575,  1012,   102,     0,     0,     0],\n",
      "        [  101,  1996,  2610,  2359,  2000,  2338,  2032,  2005,  4439,  2205,\n",
      "          3435,  1012,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2064,  3914,  9735,  2005,  1996,  4164,  2279,  2733,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I can book tickets for the concert next week.\",\n",
    "    \"Many readers find the first book of A Tale of Two Cities to be confusing.\",\n",
    "    \"She opened the book to page 37 and began to read aloud.\",\n",
    "    \"The police wanted to book him for driving too fast.\",\n",
    "    \"I can reserve tickets for the concert next week.\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")  \n",
    "\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a44fc-218e-462d-86ef-fee203417bb6",
   "metadata": {},
   "source": [
    "`model_inputs` is a dictionary containing three objects:\n",
    " * The `input_ids` are the list of token IDs in the input sequences. \n",
    " * The `attention_mask` records which tokens are special padding tokens and which are real tokens. Tokens with a 0 in the attention mask will be ignored.\n",
    " * `token_type_ids` is needed when two sequences are passed together as input to the model for tasks such as next sentence prediction that involve comparing two sentences. Here, each input is a single sentence, so we have only one type of token in the output above. \n",
    " \n",
    "**TO-DO 4:** Look at the outputs above and work out which value the special padding tokens have? \n",
    "\n",
    "ANSWER: Pad tokens are 0. \n",
    "\n",
    "---\n",
    "\n",
    "Notice that the input_ids all start with the same token ID, 101, even though they have different first words. They also have token ID 102 before the padding tokens. This is because the tokenizer inserts two special tokens, which are used in some applicaions of BERT. 101 is the '[CLS]' token, which is a dummy token whose embedding can be trained to represent the whole sequence. The [CLS] token's embedding can then be used as input to a text classifier to classify a sentence or document. Token 102 is '[SEP]', which can be used to separate multiple input sequences in a single example. This is needed in tasks where multiple pieces of text are provided as input, e.g., a to build a classifier that can determine whether two sentences contradict each other. \n",
    "\n",
    "We can now pass all of the model inputs to the model to produce a set of contextualised embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b888b7cf-54d1-450b-9431-e64629177d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs is a dictionary, so to provide the arguments to model(), \n",
    "# we use the double star to unpack the dictionary so that each key in the dictionary is\n",
    "# an argument to model() and each value is the value of the argument. \n",
    "model_outputs = model(**model_inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8eb74-ce63-459c-9fb3-05fd2768ddf8",
   "metadata": {},
   "source": [
    "**TO-DO 5:** The first four example sentences above all contain the word \"book\", and the last example contains \"reserve\". Obtain a list of contextualised word embeddings for 'book' and 'reserve' in the example sentences using our model. \n",
    "\n",
    "Hint: you may need to convert tensors to numpy arrays. Don't forget that the sequence of embeddings contains [CLS] and [SEP] embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f0297da-4106-41ef-8f61-5aa3fbb2cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tok_id = tokenizer.convert_tokens_to_ids(['book'])\n",
    "\n",
    "#WRITE YOUR OWN CODE HERE\n",
    "embeddings = model_outputs['last_hidden_state']\n",
    "\n",
    "book_embs = []\n",
    "for i in range(4):\n",
    "    book_idx_in_sen = np.argwhere(model_inputs[\"input_ids\"][i].numpy() == book_tok_id)[0][0]\n",
    "    book_embs.append(embeddings[i][book_idx_in_sen].detach().numpy())\n",
    "\n",
    "reserve_tok_id = tokenizer.convert_tokens_to_ids(['reserve'])\n",
    "reserve_idx_in_sen = np.argwhere(model_inputs[\"input_ids\"][4].numpy() == reserve_tok_id)[0][0]\n",
    "reserve_emb = embeddings[4][reserve_idx_in_sen].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac47b72-46a3-40ee-b081-bca17da0b49b",
   "metadata": {},
   "source": [
    "**TO-DO 6:** Compute the similarities between these embeddings in the cell below, and show the results. How do the similarities relate to the meaning of the word \"book\" or \"reserve\" in each sentence?\n",
    "\n",
    "ANSWER \n",
    "\n",
    "The occurrences of 'book' with different meanings have larger cosine distances. 'reserve' has a similar meaning to 'book' in the first sentence, so has high similarity. 'book' in the third and second usages has the same meaning but the first and third are different. The fourth sentence contains 'book' as a verb rather than a noun, so has slightly lower similarity with the first.  This shows that the contextualised embeddings change depending on the sentence the word is used in, and its intended meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a9c5c1-b6b2-4df4-8729-0f0615b5e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can book tickets for the concert next week.\n",
      "Many readers find the first book of A Tale of Two Cities to be confusing.\n",
      "She opened the book to page 37 and began to read aloud.\n",
      "The police wanted to book him for driving too fast.\n",
      "I can reserve tickets for the concert next week.\n",
      "\n",
      "The table below shows similarities between words according to their contextualised embeddings:\n",
      "[[1.   0.58 0.63 0.6  0.74]\n",
      " [0.58 1.   0.75 0.59 0.29]\n",
      " [0.63 0.75 1.   0.44 0.29]\n",
      " [0.6  0.59 0.44 1.   0.5 ]\n",
      " [0.74 0.29 0.29 0.5  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist  # you may find this function useful for computing distances\n",
    "\n",
    "### WRITE YOUR ANSWER HERE\n",
    "book_embs.append(reserve_emb)\n",
    "\n",
    "similarities = 1 - cdist(book_embs, book_embs, metric='cosine')\n",
    "\n",
    "###\n",
    "\n",
    "for sen in sentences:\n",
    "    print(sen)\n",
    "    \n",
    "print()\n",
    "print(\"The table below shows similarities between words according to their contextualised embeddings:\") \n",
    "print(np.round(similarities, decimals=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d6407-24ea-41f5-a16e-d03fe15aa124",
   "metadata": {},
   "source": [
    "**TO-DO 7:** Use the BERT model to obtain an embedding of each complete sentence from the five sentences listed above. Show the similarities and discuss what you see. \n",
    "\n",
    "ANSWER: We can use the CLS token to represent the text OR take a mean of the word embeddings of each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32808677-1efb-49aa-a384-cd8ca1632d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.9157727  0.912248   0.92487733 0.99678734]\n",
      " [0.9157727  1.         0.88619383 0.94227128 0.92401025]\n",
      " [0.912248   0.88619383 1.         0.88843559 0.91287952]\n",
      " [0.92487733 0.94227128 0.88843559 1.         0.92794145]\n",
      " [0.99678734 0.92401025 0.91287952 0.92794145 1.        ]]\n",
      "The most similar sentence to \"I can reserve tickets for the concert next week.\" is \"I can book tickets for the concert next week.\", according to TinyBERT.\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR ANSWER HERE\n",
    "\n",
    "cls_embs = embeddings[:, 0].detach().numpy()\n",
    "similarities = 1 - cdist(cls_embs, cls_embs, metric='cosine')\n",
    "\n",
    "###\n",
    "\n",
    "print(similarities)\n",
    "# Let's find the most similar sentences...\n",
    "similarities[range(5), range(5)] = 0  # ignore the similarity between a sentence and itself\n",
    "most_similar = np.argmax(np.max(similarities, axis=1))\n",
    "\n",
    "print(f'The most similar sentence to \"{sentences[-1]}\" is \"{sentences[most_similar]}\", according to TinyBERT.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635306b-51de-4e9d-8608-8bd7473b294a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Transformer-based Text Classifiers\n",
    "\n",
    "In this section, you will learn how to construct and train a text classifier on top of a pretrained transformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1339d48-6225-48cd-b82c-306a89ad3342",
   "metadata": {},
   "source": [
    "To begin you will need to instantiate a suitable classifier model.\n",
    "\n",
    "**TO-DO 8:** Find an AutoModel class that constructs a text classifier from the pretrained TinyBERT model, \"huawei-noah/TinyBERT_General_4L_312D\". Create the `model` object in the cell below using this class. Refer to the [Hugging Face documentation for auto models](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99068090-6a2d-47f0-9d22-b78a6705cd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR ANSWER HERE ###\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a38d4e-27a1-4c0c-b3fa-e8dc3bdf61b5",
   "metadata": {},
   "source": [
    "Typically, sequence classification models attach a linear layer (classification head) to the outputs of the transformer. The CLS token's embedding is passed into the classification head, which makes a prediction over classes. We can see a similar structure in most neural network models. Our original text classifier from the first notebook used a fully-connected layer to produce a hidden representation of the whole sentence, whereas now we are replacing that hidden layer with a complete BERT transformer, which produces a sequence of embeddings. \n",
    "\n",
    "<img src=\"neural_text_classifier_smaller.png\" alt=\"Neural text classifier diagram from the slides in lecture 8.1\" width=\"400px\"/>\n",
    "\n",
    "\n",
    "We will need to train our model before we can use it (you may see a message in the output of the last cell telling you this). \n",
    "\n",
    "**TO-DO 9:** The classifier is built on top of a pretrained TinyBERT transformer, which was pretrained using masked language modelling and next sentence prediction. Why does the classifier require further training to provide accurate sentiment classifications? \n",
    "\n",
    "ANSWER\n",
    "\n",
    "Only the BERT layers are pretrained. The complete classifier has additional classifier head layers on top of BERT, which are initialised randomly. The pretraining tasks did not include tweet classification, so the model does not yet encode any relationship between the text embeddings and the emotion categories.  \n",
    "\n",
    "---\n",
    "\n",
    "Next, let's learn how to train our model. For some tasks it is not necessary to update the weights in the BERT model itself, so we can freeze them to save a lot of computation time. We can do this as follows. Since our pretrained model is based on BERT, we can access the weights inside BERT through the variable `model.bert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8ad56b1-d6e7-4048-9cc0-71849a003363",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0c065-fb40-4e24-b4e4-ffe9ed473ba9",
   "metadata": {},
   "source": [
    "To train our model, we can make use of the Trainer class, which encapsulates a lot of the complex training steps and avoids the need to define our own training function, as we did in the previous notebook (we don't need to write our own `train_nn`).\n",
    "\n",
    "First, define some settings for the training process. This is where we can set training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62af0a20-79a5-49ca-882a-638a68b288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"transformer_checkpoints\",  # specify the directory where models weights will be saved a certain points during training (checkpoints)\n",
    "    num_train_epochs=3, # A sensible and sufficient number to use for the to-dos below\n",
    "    per_device_train_batch_size=16,  # you can decrease this if memory usage is too high while training\n",
    "    logging_steps=50,  # how often to print progress during training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b86a0-df7f-4978-a0ff-43d1b68975c5",
   "metadata": {},
   "source": [
    "Next, create a trainer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e5ba93c-f8c4-4542-a316-6aac4a32362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch import nn\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33b30b-2c23-45e5-b2a1-757f1bf93ab7",
   "metadata": {},
   "source": [
    "To train the model, you will need to call `trainer.train()`.\n",
    "\n",
    "Once the model is trained, we can obtain predictions using the function below. Notice that it is simpler than obtaining the spans for QA -- we simply get the logits for each tweet in the test set, then apply argmax over the classes to find the most probable class for each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad4e689d-ebec-44c4-b81d-649079e4a4a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# device to run computation on\n",
    "if torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")  # for mac use with MPS\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Default to CPU\n",
    "\n",
    "def predict_nn(trained_model, test_dataset):\n",
    "    \n",
    "    # Switch off dropout\n",
    "    trained_model.eval()\n",
    "    \n",
    "    # Convert the dataset into tensors and create a DataLoader\n",
    "    batch_size = 16  # Adjust based on available memory\n",
    "    test_dataset_tensors = TensorDataset(\n",
    "        torch.tensor(test_dataset[\"input_ids\"]), \n",
    "        torch.tensor(test_dataset[\"attention_mask\"])\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset_tensors, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Store predictions\n",
    "    pred_labs = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask = [x.to(device) for x in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = trained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Get predicted labels\n",
    "            preds = np.argmax(output[\"logits\"].detach().cpu().numpy(), axis=1)\n",
    "            pred_labs.extend(preds)\n",
    "    \n",
    "    # Convert to NumPy array \n",
    "    pred_labs = np.array(pred_labs)\n",
    "    \n",
    "    return pred_labs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf8564-4f98-4e3f-b1f7-b59725a0605c",
   "metadata": {},
   "source": [
    "You should now have all the bits and pieces needed to build and train a text classifier. Let's put them all together...\n",
    "\n",
    "**TO-DO 10:** Train and test your sequence classifier on the [Sentiment](https://huggingface.co/datasets/tweet_eval) dataset using a pretrained transformer. Choose a suitable evaluation metric and compare the result with the simpler neural network classifiers from the previous lab. \n",
    "\n",
    "You may wish to 'unfreeze' the BERT model to see if this boosts performance, but note that it will require a lot more computation time to fine-tune the whole transformer model. Increasing the number of epochs could also boost performance, but again requires much more computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fe80e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 45615 instances loaded\n",
      "Validation dataset with 2000 instances loaded\n",
      "Test dataset with 12284 instances loaded\n"
     ]
    }
   ],
   "source": [
    "cache_dir = './data_cache/'\n",
    "\n",
    "# Load up the emotion dataset...\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"train\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"validation\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Validation dataset with {len(val_dataset)} instances loaded\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"test\",\n",
    "    #cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d64afba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b3cf9220aa4dc3880dd6aee72c80d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174746f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=3)\n",
    "\n",
    "# you'll get better performance on this task if you don't freeze the BERT model, or perhaps if you train for more epochs\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efb6e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "882cecde-067e-4692-80f6-b09671cd9243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72208ba77fb24a3199692cf37f8f86eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8553 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.096, 'grad_norm': 0.15513628721237183, 'learning_rate': 4.97077048988659e-05, 'epoch': 0.02}\n",
      "{'loss': 1.0906, 'grad_norm': 0.287805438041687, 'learning_rate': 4.9415409797731794e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0871, 'grad_norm': 0.23804907500743866, 'learning_rate': 4.912311469659769e-05, 'epoch': 0.05}\n",
      "{'loss': 1.0824, 'grad_norm': 0.23868264257907867, 'learning_rate': 4.8830819595463585e-05, 'epoch': 0.07}\n",
      "{'loss': 1.0809, 'grad_norm': 0.14542195200920105, 'learning_rate': 4.853852449432948e-05, 'epoch': 0.09}\n",
      "{'loss': 1.0746, 'grad_norm': 0.3537963330745697, 'learning_rate': 4.8246229393195376e-05, 'epoch': 0.11}\n",
      "{'loss': 1.0741, 'grad_norm': 0.23777759075164795, 'learning_rate': 4.795393429206127e-05, 'epoch': 0.12}\n",
      "{'loss': 1.0685, 'grad_norm': 0.31881213188171387, 'learning_rate': 4.766163919092716e-05, 'epoch': 0.14}\n",
      "{'loss': 1.064, 'grad_norm': 0.19587695598602295, 'learning_rate': 4.7369344089793056e-05, 'epoch': 0.16}\n",
      "{'loss': 1.0629, 'grad_norm': 0.15789072215557098, 'learning_rate': 4.707704898865895e-05, 'epoch': 0.18}\n",
      "{'loss': 1.0551, 'grad_norm': 0.23003645241260529, 'learning_rate': 4.678475388752485e-05, 'epoch': 0.19}\n",
      "{'loss': 1.0591, 'grad_norm': 0.2381700575351715, 'learning_rate': 4.649245878639074e-05, 'epoch': 0.21}\n",
      "{'loss': 1.0553, 'grad_norm': 0.2122127115726471, 'learning_rate': 4.620016368525664e-05, 'epoch': 0.23}\n",
      "{'loss': 1.054, 'grad_norm': 0.34392181038856506, 'learning_rate': 4.5907868584122534e-05, 'epoch': 0.25}\n",
      "{'loss': 1.0508, 'grad_norm': 0.21218861639499664, 'learning_rate': 4.561557348298843e-05, 'epoch': 0.26}\n",
      "{'loss': 1.0573, 'grad_norm': 0.22052273154258728, 'learning_rate': 4.5323278381854325e-05, 'epoch': 0.28}\n",
      "{'loss': 1.0394, 'grad_norm': 0.13138894736766815, 'learning_rate': 4.503098328072022e-05, 'epoch': 0.3}\n",
      "{'loss': 1.0478, 'grad_norm': 0.28011929988861084, 'learning_rate': 4.4738688179586116e-05, 'epoch': 0.32}\n",
      "{'loss': 1.0416, 'grad_norm': 0.22490839660167694, 'learning_rate': 4.444639307845201e-05, 'epoch': 0.33}\n",
      "{'loss': 1.0292, 'grad_norm': 0.11091132462024689, 'learning_rate': 4.415409797731791e-05, 'epoch': 0.35}\n",
      "{'loss': 1.0451, 'grad_norm': 0.18992683291435242, 'learning_rate': 4.38618028761838e-05, 'epoch': 0.37}\n",
      "{'loss': 1.0458, 'grad_norm': 0.2789054214954376, 'learning_rate': 4.35695077750497e-05, 'epoch': 0.39}\n",
      "{'loss': 1.0414, 'grad_norm': 0.3332078456878662, 'learning_rate': 4.327721267391559e-05, 'epoch': 0.4}\n",
      "{'loss': 1.0327, 'grad_norm': 0.4016497731208801, 'learning_rate': 4.298491757278148e-05, 'epoch': 0.42}\n",
      "{'loss': 1.0391, 'grad_norm': 0.3141988515853882, 'learning_rate': 4.269262247164738e-05, 'epoch': 0.44}\n",
      "{'loss': 1.033, 'grad_norm': 0.3318251669406891, 'learning_rate': 4.2400327370513274e-05, 'epoch': 0.46}\n",
      "{'loss': 1.0336, 'grad_norm': 0.06631307303905487, 'learning_rate': 4.210803226937917e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0382, 'grad_norm': 0.16450285911560059, 'learning_rate': 4.1815737168245065e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0241, 'grad_norm': 0.23255054652690887, 'learning_rate': 4.152344206711096e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0282, 'grad_norm': 0.15756697952747345, 'learning_rate': 4.123114696597685e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0287, 'grad_norm': 0.14779476821422577, 'learning_rate': 4.0938851864842745e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0174, 'grad_norm': 0.24845945835113525, 'learning_rate': 4.064655676370864e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0283, 'grad_norm': 0.19642551243305206, 'learning_rate': 4.0354261662574536e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0156, 'grad_norm': 0.28640368580818176, 'learning_rate': 4.006196656144043e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0267, 'grad_norm': 0.26397886872291565, 'learning_rate': 3.976967146030633e-05, 'epoch': 0.61}\n",
      "{'loss': 1.023, 'grad_norm': 0.2668693959712982, 'learning_rate': 3.947737635917222e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0203, 'grad_norm': 0.09728273004293442, 'learning_rate': 3.918508125803812e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0283, 'grad_norm': 0.0872647687792778, 'learning_rate': 3.889278615690401e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0254, 'grad_norm': 0.20549610257148743, 'learning_rate': 3.86004910557699e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0112, 'grad_norm': 0.05744187533855438, 'learning_rate': 3.83081959546358e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0175, 'grad_norm': 0.19163338840007782, 'learning_rate': 3.8015900853501694e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0182, 'grad_norm': 0.1771445870399475, 'learning_rate': 3.772360575236759e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0206, 'grad_norm': 0.18095889687538147, 'learning_rate': 3.7431310651233485e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0094, 'grad_norm': 0.26451995968818665, 'learning_rate': 3.713901555009938e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0075, 'grad_norm': 0.26247653365135193, 'learning_rate': 3.6846720448965276e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0144, 'grad_norm': 0.05504942312836647, 'learning_rate': 3.655442534783117e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0022, 'grad_norm': 0.2787916660308838, 'learning_rate': 3.626213024669707e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0248, 'grad_norm': 0.2901892364025116, 'learning_rate': 3.596983514556296e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0038, 'grad_norm': 0.1221843808889389, 'learning_rate': 3.567754004442886e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0122, 'grad_norm': 0.43290597200393677, 'learning_rate': 3.5385244943294754e-05, 'epoch': 0.88}\n",
      "{'loss': 1.0132, 'grad_norm': 0.055942993611097336, 'learning_rate': 3.509294984216065e-05, 'epoch': 0.89}\n",
      "{'loss': 1.0098, 'grad_norm': 0.11878679692745209, 'learning_rate': 3.480065474102654e-05, 'epoch': 0.91}\n",
      "{'loss': 1.0223, 'grad_norm': 0.18596982955932617, 'learning_rate': 3.4508359639892434e-05, 'epoch': 0.93}\n",
      "{'loss': 1.0167, 'grad_norm': 0.26633986830711365, 'learning_rate': 3.421606453875833e-05, 'epoch': 0.95}\n",
      "{'loss': 1.0048, 'grad_norm': 0.10746410489082336, 'learning_rate': 3.3923769437624225e-05, 'epoch': 0.96}\n",
      "{'loss': 1.0092, 'grad_norm': 0.20826789736747742, 'learning_rate': 3.363147433649012e-05, 'epoch': 0.98}\n",
      "{'loss': 1.0045, 'grad_norm': 0.12257955968379974, 'learning_rate': 3.3339179235356016e-05, 'epoch': 1.0}\n",
      "{'loss': 1.0169, 'grad_norm': 0.19794124364852905, 'learning_rate': 3.304688413422191e-05, 'epoch': 1.02}\n",
      "{'loss': 1.0095, 'grad_norm': 0.2025376409292221, 'learning_rate': 3.275458903308781e-05, 'epoch': 1.03}\n",
      "{'loss': 1.0172, 'grad_norm': 0.30864784121513367, 'learning_rate': 3.24622939319537e-05, 'epoch': 1.05}\n",
      "{'loss': 1.007, 'grad_norm': 0.19710774719715118, 'learning_rate': 3.21699988308196e-05, 'epoch': 1.07}\n",
      "{'loss': 1.0229, 'grad_norm': 0.17363497614860535, 'learning_rate': 3.1877703729685494e-05, 'epoch': 1.09}\n",
      "{'loss': 1.0072, 'grad_norm': 0.2642866373062134, 'learning_rate': 3.158540862855139e-05, 'epoch': 1.1}\n",
      "{'loss': 0.9957, 'grad_norm': 0.10488350689411163, 'learning_rate': 3.1293113527417285e-05, 'epoch': 1.12}\n",
      "{'loss': 1.0104, 'grad_norm': 0.11029546707868576, 'learning_rate': 3.100081842628318e-05, 'epoch': 1.14}\n",
      "{'loss': 1.0241, 'grad_norm': 0.25467121601104736, 'learning_rate': 3.0708523325149076e-05, 'epoch': 1.16}\n",
      "{'loss': 1.0154, 'grad_norm': 0.11997868120670319, 'learning_rate': 3.0416228224014965e-05, 'epoch': 1.18}\n",
      "{'loss': 1.0013, 'grad_norm': 0.25556373596191406, 'learning_rate': 3.012393312288086e-05, 'epoch': 1.19}\n",
      "{'loss': 1.0228, 'grad_norm': 0.1768902689218521, 'learning_rate': 2.9831638021746756e-05, 'epoch': 1.21}\n",
      "{'loss': 0.9964, 'grad_norm': 0.2514679729938507, 'learning_rate': 2.953934292061265e-05, 'epoch': 1.23}\n",
      "{'loss': 1.0165, 'grad_norm': 0.10548616200685501, 'learning_rate': 2.9247047819478547e-05, 'epoch': 1.25}\n",
      "{'loss': 0.9973, 'grad_norm': 0.39200299978256226, 'learning_rate': 2.8954752718344443e-05, 'epoch': 1.26}\n",
      "{'loss': 1.0206, 'grad_norm': 0.2522929608821869, 'learning_rate': 2.8662457617210338e-05, 'epoch': 1.28}\n",
      "{'loss': 1.0081, 'grad_norm': 0.30174434185028076, 'learning_rate': 2.837016251607623e-05, 'epoch': 1.3}\n",
      "{'loss': 1.0252, 'grad_norm': 0.2649032175540924, 'learning_rate': 2.8077867414942126e-05, 'epoch': 1.32}\n",
      "{'loss': 1.0017, 'grad_norm': 0.41938260197639465, 'learning_rate': 2.778557231380802e-05, 'epoch': 1.33}\n",
      "{'loss': 1.0103, 'grad_norm': 0.16312530636787415, 'learning_rate': 2.7493277212673917e-05, 'epoch': 1.35}\n",
      "{'loss': 1.0103, 'grad_norm': 0.09814875572919846, 'learning_rate': 2.7200982111539813e-05, 'epoch': 1.37}\n",
      "{'loss': 1.0031, 'grad_norm': 0.10560441017150879, 'learning_rate': 2.6908687010405708e-05, 'epoch': 1.39}\n",
      "{'loss': 0.9976, 'grad_norm': 0.267372727394104, 'learning_rate': 2.6616391909271604e-05, 'epoch': 1.4}\n",
      "{'loss': 0.9998, 'grad_norm': 0.1965991109609604, 'learning_rate': 2.63240968081375e-05, 'epoch': 1.42}\n",
      "{'loss': 0.9911, 'grad_norm': 0.11530458927154541, 'learning_rate': 2.6031801707003388e-05, 'epoch': 1.44}\n",
      "{'loss': 0.9962, 'grad_norm': 0.09782978892326355, 'learning_rate': 2.5739506605869284e-05, 'epoch': 1.46}\n",
      "{'loss': 0.9878, 'grad_norm': 0.18354955315589905, 'learning_rate': 2.544721150473518e-05, 'epoch': 1.47}\n",
      "{'loss': 1.016, 'grad_norm': 0.41662806272506714, 'learning_rate': 2.5154916403601075e-05, 'epoch': 1.49}\n",
      "{'loss': 0.999, 'grad_norm': 0.1691526174545288, 'learning_rate': 2.486262130246697e-05, 'epoch': 1.51}\n",
      "{'loss': 1.0287, 'grad_norm': 0.1802334040403366, 'learning_rate': 2.4570326201332866e-05, 'epoch': 1.53}\n",
      "{'loss': 1.0289, 'grad_norm': 0.0919375866651535, 'learning_rate': 2.427803110019876e-05, 'epoch': 1.54}\n",
      "{'loss': 0.9893, 'grad_norm': 0.24390676617622375, 'learning_rate': 2.3985735999064657e-05, 'epoch': 1.56}\n",
      "{'loss': 0.996, 'grad_norm': 0.18774718046188354, 'learning_rate': 2.3693440897930553e-05, 'epoch': 1.58}\n",
      "{'loss': 1.0066, 'grad_norm': 0.21956874430179596, 'learning_rate': 2.3401145796796448e-05, 'epoch': 1.6}\n",
      "{'loss': 1.0097, 'grad_norm': 0.2484348565340042, 'learning_rate': 2.3108850695662344e-05, 'epoch': 1.61}\n",
      "{'loss': 0.9964, 'grad_norm': 0.2046651989221573, 'learning_rate': 2.2816555594528236e-05, 'epoch': 1.63}\n",
      "{'loss': 0.9883, 'grad_norm': 0.053975481539964676, 'learning_rate': 2.252426049339413e-05, 'epoch': 1.65}\n",
      "{'loss': 1.0289, 'grad_norm': 0.10973008722066879, 'learning_rate': 2.2231965392260027e-05, 'epoch': 1.67}\n",
      "{'loss': 1.0037, 'grad_norm': 0.26843103766441345, 'learning_rate': 2.1939670291125923e-05, 'epoch': 1.68}\n",
      "{'loss': 0.9829, 'grad_norm': 0.17448215186595917, 'learning_rate': 2.1647375189991818e-05, 'epoch': 1.7}\n",
      "{'loss': 1.0013, 'grad_norm': 0.3283042311668396, 'learning_rate': 2.1355080088857714e-05, 'epoch': 1.72}\n",
      "{'loss': 1.0105, 'grad_norm': 0.13314808905124664, 'learning_rate': 2.106278498772361e-05, 'epoch': 1.74}\n",
      "{'loss': 1.0043, 'grad_norm': 0.2561744749546051, 'learning_rate': 2.07704898865895e-05, 'epoch': 1.75}\n",
      "{'loss': 1.0335, 'grad_norm': 0.49048393964767456, 'learning_rate': 2.0478194785455397e-05, 'epoch': 1.77}\n",
      "{'loss': 0.9955, 'grad_norm': 0.09460175782442093, 'learning_rate': 2.0185899684321293e-05, 'epoch': 1.79}\n",
      "{'loss': 1.034, 'grad_norm': 0.26438045501708984, 'learning_rate': 1.9893604583187188e-05, 'epoch': 1.81}\n",
      "{'loss': 1.0013, 'grad_norm': 0.3667098879814148, 'learning_rate': 1.960130948205308e-05, 'epoch': 1.82}\n",
      "{'loss': 1.0018, 'grad_norm': 0.19900886714458466, 'learning_rate': 1.9309014380918976e-05, 'epoch': 1.84}\n",
      "{'loss': 0.9909, 'grad_norm': 0.13433818519115448, 'learning_rate': 1.901671927978487e-05, 'epoch': 1.86}\n",
      "{'loss': 1.0036, 'grad_norm': 0.24641506373882294, 'learning_rate': 1.8724424178650767e-05, 'epoch': 1.88}\n",
      "{'loss': 0.9966, 'grad_norm': 0.1126595064997673, 'learning_rate': 1.843212907751666e-05, 'epoch': 1.89}\n",
      "{'loss': 0.999, 'grad_norm': 0.15931108593940735, 'learning_rate': 1.8139833976382555e-05, 'epoch': 1.91}\n",
      "{'loss': 1.0178, 'grad_norm': 0.20791256427764893, 'learning_rate': 1.784753887524845e-05, 'epoch': 1.93}\n",
      "{'loss': 1.007, 'grad_norm': 0.25719574093818665, 'learning_rate': 1.7555243774114346e-05, 'epoch': 1.95}\n",
      "{'loss': 1.0, 'grad_norm': 0.18639864027500153, 'learning_rate': 1.726294867298024e-05, 'epoch': 1.96}\n",
      "{'loss': 1.0241, 'grad_norm': 0.3335345983505249, 'learning_rate': 1.6970653571846137e-05, 'epoch': 1.98}\n",
      "{'loss': 0.9786, 'grad_norm': 0.33455151319503784, 'learning_rate': 1.6678358470712033e-05, 'epoch': 2.0}\n",
      "{'loss': 1.0161, 'grad_norm': 0.4589490592479706, 'learning_rate': 1.6386063369577925e-05, 'epoch': 2.02}\n",
      "{'loss': 1.0203, 'grad_norm': 0.18092693388462067, 'learning_rate': 1.609376826844382e-05, 'epoch': 2.03}\n",
      "{'loss': 1.0068, 'grad_norm': 0.23861253261566162, 'learning_rate': 1.5801473167309716e-05, 'epoch': 2.05}\n",
      "{'loss': 1.0219, 'grad_norm': 0.2980688214302063, 'learning_rate': 1.550917806617561e-05, 'epoch': 2.07}\n",
      "{'loss': 1.005, 'grad_norm': 0.08510009199380875, 'learning_rate': 1.5216882965041507e-05, 'epoch': 2.09}\n",
      "{'loss': 1.0009, 'grad_norm': 0.08744148164987564, 'learning_rate': 1.4924587863907403e-05, 'epoch': 2.1}\n",
      "{'loss': 0.9974, 'grad_norm': 0.1778828352689743, 'learning_rate': 1.4632292762773298e-05, 'epoch': 2.12}\n",
      "{'loss': 0.9921, 'grad_norm': 0.08630558103322983, 'learning_rate': 1.4339997661639194e-05, 'epoch': 2.14}\n",
      "{'loss': 0.9929, 'grad_norm': 0.06588994711637497, 'learning_rate': 1.4047702560505086e-05, 'epoch': 2.16}\n",
      "{'loss': 1.0004, 'grad_norm': 0.24976450204849243, 'learning_rate': 1.3755407459370981e-05, 'epoch': 2.17}\n",
      "{'loss': 0.9846, 'grad_norm': 0.2755900025367737, 'learning_rate': 1.3463112358236877e-05, 'epoch': 2.19}\n",
      "{'loss': 0.9897, 'grad_norm': 0.09225234389305115, 'learning_rate': 1.3170817257102771e-05, 'epoch': 2.21}\n",
      "{'loss': 1.0133, 'grad_norm': 0.2717154920101166, 'learning_rate': 1.2878522155968666e-05, 'epoch': 2.23}\n",
      "{'loss': 1.0021, 'grad_norm': 0.1681526154279709, 'learning_rate': 1.2586227054834562e-05, 'epoch': 2.24}\n",
      "{'loss': 1.006, 'grad_norm': 0.2713915705680847, 'learning_rate': 1.2293931953700456e-05, 'epoch': 2.26}\n",
      "{'loss': 0.9925, 'grad_norm': 0.160451740026474, 'learning_rate': 1.2001636852566351e-05, 'epoch': 2.28}\n",
      "{'loss': 0.9904, 'grad_norm': 0.24781200289726257, 'learning_rate': 1.1709341751432247e-05, 'epoch': 2.3}\n",
      "{'loss': 0.9892, 'grad_norm': 0.08523744344711304, 'learning_rate': 1.1417046650298141e-05, 'epoch': 2.31}\n",
      "{'loss': 0.9974, 'grad_norm': 0.1120191365480423, 'learning_rate': 1.1124751549164036e-05, 'epoch': 2.33}\n",
      "{'loss': 1.0092, 'grad_norm': 0.17288200557231903, 'learning_rate': 1.0832456448029932e-05, 'epoch': 2.35}\n",
      "{'loss': 1.0068, 'grad_norm': 0.08889137953519821, 'learning_rate': 1.0540161346895828e-05, 'epoch': 2.37}\n",
      "{'loss': 1.0052, 'grad_norm': 0.08919345587491989, 'learning_rate': 1.0247866245761721e-05, 'epoch': 2.39}\n",
      "{'loss': 1.0017, 'grad_norm': 0.2697228491306305, 'learning_rate': 9.955571144627617e-06, 'epoch': 2.4}\n",
      "{'loss': 1.0031, 'grad_norm': 0.2917060852050781, 'learning_rate': 9.663276043493513e-06, 'epoch': 2.42}\n",
      "{'loss': 0.9922, 'grad_norm': 0.09570436179637909, 'learning_rate': 9.370980942359407e-06, 'epoch': 2.44}\n",
      "{'loss': 0.9872, 'grad_norm': 0.08936630934476852, 'learning_rate': 9.0786858412253e-06, 'epoch': 2.46}\n",
      "{'loss': 1.0025, 'grad_norm': 0.17882558703422546, 'learning_rate': 8.786390740091196e-06, 'epoch': 2.47}\n",
      "{'loss': 1.0072, 'grad_norm': 0.1447361707687378, 'learning_rate': 8.494095638957092e-06, 'epoch': 2.49}\n",
      "{'loss': 0.9955, 'grad_norm': 0.34715473651885986, 'learning_rate': 8.201800537822985e-06, 'epoch': 2.51}\n",
      "{'loss': 1.0016, 'grad_norm': 0.05426229164004326, 'learning_rate': 7.909505436688881e-06, 'epoch': 2.53}\n",
      "{'loss': 0.995, 'grad_norm': 0.09085334837436676, 'learning_rate': 7.6172103355547765e-06, 'epoch': 2.54}\n",
      "{'loss': 1.0046, 'grad_norm': 0.23146508634090424, 'learning_rate': 7.324915234420672e-06, 'epoch': 2.56}\n",
      "{'loss': 1.0095, 'grad_norm': 0.14174960553646088, 'learning_rate': 7.032620133286566e-06, 'epoch': 2.58}\n",
      "{'loss': 0.9936, 'grad_norm': 0.17318452894687653, 'learning_rate': 6.7403250321524615e-06, 'epoch': 2.6}\n",
      "{'loss': 1.0149, 'grad_norm': 0.11055099219083786, 'learning_rate': 6.448029931018357e-06, 'epoch': 2.61}\n",
      "{'loss': 1.0278, 'grad_norm': 0.3032509386539459, 'learning_rate': 6.155734829884252e-06, 'epoch': 2.63}\n",
      "{'loss': 1.0122, 'grad_norm': 0.19544707238674164, 'learning_rate': 5.8634397287501465e-06, 'epoch': 2.65}\n",
      "{'loss': 0.9843, 'grad_norm': 0.33446863293647766, 'learning_rate': 5.571144627616041e-06, 'epoch': 2.67}\n",
      "{'loss': 1.0071, 'grad_norm': 0.23227928578853607, 'learning_rate': 5.278849526481936e-06, 'epoch': 2.68}\n",
      "{'loss': 0.9862, 'grad_norm': 0.23947612941265106, 'learning_rate': 4.9865544253478315e-06, 'epoch': 2.7}\n",
      "{'loss': 1.0155, 'grad_norm': 0.3251684606075287, 'learning_rate': 4.694259324213726e-06, 'epoch': 2.72}\n",
      "{'loss': 1.0023, 'grad_norm': 0.4129279553890228, 'learning_rate': 4.401964223079622e-06, 'epoch': 2.74}\n",
      "{'loss': 0.996, 'grad_norm': 0.3352162539958954, 'learning_rate': 4.1096691219455165e-06, 'epoch': 2.75}\n",
      "{'loss': 1.0069, 'grad_norm': 0.45465412735939026, 'learning_rate': 3.817374020811411e-06, 'epoch': 2.77}\n",
      "{'loss': 0.9951, 'grad_norm': 0.24340325593948364, 'learning_rate': 3.5250789196773064e-06, 'epoch': 2.79}\n",
      "{'loss': 0.9976, 'grad_norm': 0.15638704597949982, 'learning_rate': 3.2327838185432015e-06, 'epoch': 2.81}\n",
      "{'loss': 1.0049, 'grad_norm': 0.08186477422714233, 'learning_rate': 2.9404887174090963e-06, 'epoch': 2.82}\n",
      "{'loss': 1.0097, 'grad_norm': 0.1663125604391098, 'learning_rate': 2.6481936162749914e-06, 'epoch': 2.84}\n",
      "{'loss': 0.996, 'grad_norm': 0.08710886538028717, 'learning_rate': 2.355898515140886e-06, 'epoch': 2.86}\n",
      "{'loss': 0.9982, 'grad_norm': 0.2595749497413635, 'learning_rate': 2.0636034140067813e-06, 'epoch': 2.88}\n",
      "{'loss': 0.9875, 'grad_norm': 0.2720973491668701, 'learning_rate': 1.7713083128726764e-06, 'epoch': 2.89}\n",
      "{'loss': 1.0049, 'grad_norm': 0.20115351676940918, 'learning_rate': 1.4790132117385713e-06, 'epoch': 2.91}\n",
      "{'loss': 0.9981, 'grad_norm': 0.1716652512550354, 'learning_rate': 1.1867181106044663e-06, 'epoch': 2.93}\n",
      "{'loss': 1.0088, 'grad_norm': 0.14117948710918427, 'learning_rate': 8.944230094703613e-07, 'epoch': 2.95}\n",
      "{'loss': 1.0163, 'grad_norm': 0.057894933968782425, 'learning_rate': 6.021279083362562e-07, 'epoch': 2.96}\n",
      "{'loss': 1.0046, 'grad_norm': 0.14457356929779053, 'learning_rate': 3.0983280720215133e-07, 'epoch': 2.98}\n",
      "{'loss': 1.0033, 'grad_norm': 0.20871278643608093, 'learning_rate': 1.75377060680463e-08, 'epoch': 3.0}\n",
      "{'train_runtime': 203.9058, 'train_samples_per_second': 671.119, 'train_steps_per_second': 41.946, 'train_loss': 1.0150284127130935, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8553, training_loss=1.0150284127130935, metrics={'train_runtime': 203.9058, 'train_samples_per_second': 671.119, 'train_steps_per_second': 41.946, 'total_flos': 490587879916800.0, 'train_loss': 1.0150284127130935, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train...\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b68b651-4b1f-4b49-9d6e-39945933820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prediction function to get the results:\n",
    "pred_labs_frozen = predict_nn(model, test_dataset)\n",
    "\n",
    "gold_labs = test_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f2290ff-1ad2-44a0-9247-a323694815b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROZEN MODEL F1 = 0.2576930538958548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(np.array(gold_labs).flatten(), pred_labs_frozen.flatten(), average='macro')\n",
    "print(f'FROZEN MODEL F1 = {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b744785-39a5-4c84-8813-78507e37cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROZEN MODEL ACCURACY = 0.4937316834907196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(np.array(gold_labs).flatten(), pred_labs_frozen.flatten())\n",
    "print(f'FROZEN MODEL ACCURACY = {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1661d-a203-42ee-8681-025f2bdfe661",
   "metadata": {},
   "source": [
    "**TO-DO 11:** What kinds of _transfer_ did your sentiment classifier use and what benefit do they provide? \n",
    "\n",
    "ANSWER\n",
    "\n",
    "The BERT layers of the model are first pretrained on MLM and NSP tasks on a different dataset. With frozen BERT, we perform direct transfer of the BERT model to our emotion classification task. When BERT is unfrozen, we fine-tune the BERT layers, which performs inductive transfer learning. The benefit is that knowledgeabout how to process sequences of text to extract embeddings is transferred from the pretraining task (which had lots of data available) to the downstream target task (hate classification, with only a few thousand examples). \n",
    "\n",
    "---\n",
    "\n",
    "The model currently outputs logits, rather than probabilities, which are much more useful for most applications of a text classifier.  To compute the probability of each class for a test sentence, we need to pass the logits through the softmax function. Complete the function below to obtain a probability distribution for a sentence of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11fcdc69-ce30-4eb3-afc5-4eeba41b40db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of each sentiment class is:\n",
      "probability of non-hate = 0.15498974919319153\n",
      "probability of hate = 0.39903175830841064\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"A very joyful and happy day\"]\n",
    "\n",
    "model.eval()\n",
    "output = model(**tokenizer(sentences,  max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device))\n",
    "        \n",
    "# the output dictionary contains logits, which are the unnormalised scores for each class for each example:\n",
    "logits = output[\"logits\"]\n",
    "\n",
    "#### WRITE YOUR ANSWER HERE   \n",
    "probs = torch.nn.Softmax(dim=1)(output[\"logits\"])\n",
    "####\n",
    "\n",
    "print(f'The probability of each sentiment class is:')\n",
    "classes = ['non-hate', 'hate']\n",
    "for c, category in enumerate(classes):\n",
    "    print(f'probability of {category} = {probs[0][c].detach().cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb0d80-4624-4690-a94d-b9f3c1a60d7f",
   "metadata": {},
   "source": [
    "# 3. OPTIONAL: More on Transformers\n",
    "\n",
    "There are many great resources out there to show you how to use this kind of model in practice:\n",
    "* Use a Transformer for sequence tagging by following the [Token Classification tutorial](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=vc0BSBLIIrJQ) from HuggingFace. This is a little more involved than sequence classification because the tags provided in the training dataset require the text to be tokenized in a particular way, which often differs from what a particular pretrained transformer requires.\n",
    "* An extensive online course is provided by HuggingFace: https://huggingface.co/course/chapter1/1. The pages linked from the HuggingFace course website have an 'open in Colab' button on the top right. You can open the notebook and run it on a Google server there to access GPUs.\n",
    "* Chapters that may be particularly useful: \n",
    "   * Transformers, what can they do? https://huggingface.co/course/chapter1/3?fw=pt\n",
    "   * Using Transformers: https://huggingface.co/course/chapter2/2?fw=pt\n",
    "* They provide information on fine-tuning the transformer models here: https://huggingface.co/docs/transformers/training. Fine-tuning updates the weights inside the pretrained network and requires extensive GPU or TPU computing. \n",
    "* Text Generation: https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb. This topic goes way beyond data analytics on this unit and shows you another powerful feature of pretrained transformers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e18f91-4107-437a-b939-caf4c321e7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
